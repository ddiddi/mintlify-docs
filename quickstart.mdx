## SoloÂ Server Setup Guide

<div align="center" >

<div style={{ display: "flex", justifyContent: "center" }}>
  <img alt="Solo Light" title="Solo Logo" classname="block dark:hidden" src="/SOLO(2).jpg" />
</div>

<div style={{ display: "flex", justifyContent: "center", alignItems: "center", gap: "40px", flexWrap: "wrap", marginTop: "20px" }}>
  <a href="https://www.python.org/downloads/">
    <img src="https://img.shields.io/badge/Python-3.9%2B-blue.svg" alt="Python 3.9+" />
  </a>
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/pypi/l/solo-server" alt="License: MIT" />
  </a>
  <a href="https://pypi.org/project/solo-server/">
    <img src="https://img.shields.io/pypi/dm/solo-server" alt="PyPI - Downloads" />
  </a>
  <a href="https://pypi.org/project/solo-server/">
    <img src="https://img.shields.io/pypi/v/solo-server" alt="PyPI - Version" />
  </a>
</div>



**SoloÂ Server** is a lightweight orchestration layer for **hardwareâ€‘aware inference**. Spin up Ollama,Â vLLM, orÂ Llama.cpp backâ€‘ends in seconds with an opinionated CLI and a consistent REST API.

</div>

---

```bash
# Install
pip install solo-server

# Interactive setup (detects hardware, writes solo.json)
solo setup
```

<div align="center" style={{ marginTop: "20px" }}>
  <video width="900" controls>
    <source src="/solo.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</div>

---

## âœ¨Â Features

| | |
|---|---|
| âš¡ **Seamless setup** | Oneâ€‘command `solo setup` autoâ€‘detects CPU/GPU/RAM and writes an optimised config |
| ğŸ“š **Open model registry** | Pull weights from HuggingÂ Face, Ollama, or local GGUF bins |
| ğŸ–¥ï¸ **Crossâ€‘platform** | macOSÂ (AppleÂ Silicon & Intel), Linux, WindowsÂ 10/11 |
| ğŸ› ï¸ **Configurable framework** | Tweak ports, backâ€‘end, quantisation, & device mapping in `~/.solo_server/solo.json` |

---

## TableÂ ofÂ Contents  
- [Installation](#installation)  
- [Commands](#commands)  
- [RESTÂ API](#rest-api)  
- [Configuration](#ï¸-configuration-solojson)  
- [Project inspiration](#-project-inspiration)

---

## Installation

### ğŸ”¹Â Prerequisites
- **Docker**Â ğŸ³ â€” required for container backâ€‘ends. [InstallÂ Docker](https://docs.docker.com/get-docker/)

### ğŸ”¹Â Install with **uv**Â (recommended)
```bash
# Install uv (see full instructions: https://docs.astral.sh/uv/getting-started/installation/)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create and activate a virtualenv
uv venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows PowerShell

# Install SoloÂ Server
uv pip install solo-server

# Run the interactive wizard
solo setup
```
The wizard **detects hardware**, **selects the optimal compute backâ€‘end** (CUDA, HIP,Â Metal,Â CPU, â€¦) and writes `solo.json`.

---

## SoloÂ ServerÂ BlockÂ Diagram
<div align="center">
  <img src="assets/Solo Server.svg" width="900" />
</div>

---

## Commands

### Serve a model
```bash
solo serve -s ollama -m llama3.2
```

| Flag | Description | Default |
|------|-------------|---------|
| `-s,Â --server` | Backâ€‘end: `ollama`, `vllm`, `llama.cpp` | `ollama` |
| `-m,Â --model`  | Model name or path | â€” |
| `-p,Â --port`   | HTTP port | 5070 |

### Test inference
```bash
solo test            # quick healthâ€‘check
solo test --timeout 120  # increase timeout for large models
```

### List models
```bash
solo list   # scans HuggingÂ Face cache & Ollama store
```

### Check server status
```bash
solo status
```

### Stop servers
```bash
solo stop   # gracefully shutdown running backâ€‘ends
```

---

## REST API
Solo exposes a thin proxy so your code never needs to change when you swap backâ€‘ends.

### Ollamaâ€‘style endpoints
```bash
curl http://localhost:5070/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?"
}'
```

### OpenAIâ€‘compatible endpoints (vLLM & Llama.cpp)
```bash
curl http://localhost:5070/v1/chat/completions -d '{
  "model": "llama3.2",
  "messages": [{"role": "user", "content": "Why is the sky blue?"}]
}'
```

---

## âš™ï¸ Configuration (`solo.json`)
`solo setup` writes a machineâ€‘specific config at `~/.solo_server/solo.json`. Edit it manually or rerun the wizard any time.
```json
{
  "hardware": {
    "use_gpu": true,
    "compute_backend": "CUDA",
    "gpu_memory": 6144.0
  },
  "server": {"type": "ollama", "default_port": 5070},
  "active_model": {"server": "ollama", "name": "llama3.2:1b"}
}
```

---

## ğŸ“Â Project inspiration
SoloÂ Server stands on the shoulders of:
- **uv**Â â€“ blazingâ€‘fast Python package manager
- **llama.cpp**, **vLLM**, **Ollama** â€“ stateâ€‘ofâ€‘theâ€‘art inference backâ€‘ends
- **HuggingÂ Face Hub**, **whisper.cpp**, **llamafile**, **podman**, **cog**

If you find Solo useful, please â­ the repo!

