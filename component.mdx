---
title: "Recommended Models"
description: "Guide to choosing the right LLM model based on your hardware, with recommendations from 3GB to 14GB+ GPU RAM requirements"
---

---

> **Tip**  If a model you need isn’t listed, try it anyway—`solo serve` will attempt to pick the right back‑end automatically. The only hard requirement is that the architecture is supported by one of **Ollama**, **vLLM**, or **Llama.cpp**.

## Quick‑look compatibility matrix

| Family                    | Sizes         | Ollama | vLLM (FP16/BF16) | Llama.cpp (GGUF) | Licence                |
| :------------------------ | :------------ | :----- | :--------------- | :--------------- | :--------------------- |
| **Llama 3**               | 8B, 70B       | ✔️     | ✔️               | Q4_K_M–Q8_0      | Meta Llama 3 Community |
| **Gemma 2**               | 2B, 7B        | ✔️     | ✔️               | Q4_K_M–Q8_0      | Apache‑2.0             |
| **DeepSeek R‑series**     | 1B, 7B        | ✔️     | ✔️               | Q4_K_M–Q8_0      | MIT                    |
| **Mistral / Mixtral**     | 7B, 8x7B      | ✔️     | ✔️               | Q4_K_M–Q8_0      | Apache‑2.0             |
| **Phi‑3‑mini**            | 3.8B          | ✔️     | ✔️               | Q4_K_M–Q8_0      | MIT                    |
| **Qwen 1.5**              | 4B, 7B, 14B   | ✔️     | ✔️               | Q4_K_M–Q8_0      | Apache‑2.0             |
| **StableLM 2**            | 1.6B, 12B     | ✔️     | ✔️               | Q4_K_M–Q8_0      | CC‑BY‑SA‑4.0           |
| **Code LLM (StarCoder2)** | 3B, 7B, 15B   | ✔️     | ✔️               | Q4_K_M–Q8_0      | BigCode OpenRAIL‑M     |
| **Audio / Whisper cpp**   | Tiny‑large‑v3 | n/a    | n/a              | ✔️               | MIT                    |

Legend: **✔️** = fully supported; _blank_ = not recommended; _n/a_ = not applicable.

---

## Recommended starter set

| Use‑case                                   | Model                          | Size (params) | RAM needed\* | Notes                            |
| :----------------------------------------- | :----------------------------- | :------------ | :----------- | :------------------------------- |
| General chat on laptop                     | **Llama 3 8B‑Instruct Q4_K_M** | 8 B           | ≈6 GB        | Best accuracy/VRAM ratio.        |
| Edge devices (Raspberry Pi 5, Jetson Nano) | **Phi‑3‑mini Q4_K_M**          | 3.8 B         | ≈3 GB        | Fast and small; good reasoning.  |
| Private code‑assist                        | **StarCoder2 7B GGUF**         | 7 B           | ≈5 GB        | Optimised tokenizer for code.    |
| Knowledge RAG                              | **DeepSeek‑R1‑1.5B Q5_K_M**    | 1.5 B         | ≈1.4 GB      | Tiny footprint, solid retrieval. |
| Multilingual chat                          | **Gemma 7B‑Instruct**          | 7 B           | ≈14 GB FP16  | Strong non‑English coverage.     |

\*RAM figures assume quantised GGUF on CPU. GPU VRAM ≈ same as file size.

---

## Bringing your own model

```
# Any HF repo that vLLM supports (AutoModelForCausalLM)
solo serve -s vllm -m mistralai/Mistral-7B-Instruct-v0.3

# Any GGUF file in your cache directory
solo serve -s llama.cpp -m ~/.cache/huggingface/hub/.../phi-3-mini-it-q4_k_m.gguf

# Any Ollama package (remote pull on first use)
solo serve -s ollama -m llama3
```

If the model fails to load,

1. Check that the model’s **architecture** is supported by the back‑end.
2. Verify your **VRAM/RAM** meets the model’s requirements.
3. Re‑run `solo setup` to regenerate an optimised config.

---

## Quantisation cheat‑sheet (Llama.cpp)

| Suffix     | Bits / weight | Typical accuracy drop | When to use                       |
| :--------- | :------------ | :-------------------- | :-------------------------------- |
| **Q2_K**   | 2             | 3‑6 pp                | Ultra‑low memory (≤2 GB)          |
| **Q3_K_S** | 3             | 2‑4 pp                | Low‑RAM edge devices              |
| **Q4_K_M** | 4             | ≤2 pp                 | Balanced; default pick            |
| **Q5_K_M** | 5             | \<1 pp                | Still small, better quality       |
| **Q8_0**   | 8             | ~0 pp                 | GPU inference with plenty of VRAM |

---

## FAQ

### _Can I run 70B\+ models on a 16 GB laptop?_

Not realistically. Even the most aggressive quantisation needs ~30 GB RAM. Try 7‑8 B models or use **vLLM** with tensor parallel GPUs.

### _Does Solo support fine‑tuned LoRA adapters?_

Yes—point to a `.safetensors` adapter with `--lora path/to/adapter` (Llama.cpp) or use `vllm --lora-adapter`.

### _What about vision or audio models?_

Solo’s focus is text generation. Vision encoders and Whisper transcription load fine with Llama.cpp builds that include **BLAS \+ SSL**.

---

Have a model request or problem loading a checkpoint? [Open an issue](https://github.com/GetSoloTech/solo-server/issues) and we’ll add first‑class support.