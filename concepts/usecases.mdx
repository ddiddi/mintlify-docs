---
title: 'Usecase-Based Model Selection'
description: 'Learn how to use predefined usecases and model shortnames for easier model deployment'
icon: 'list-check'
---

## Overview

Solo Server supports a usecase-based approach to model selection, allowing you to deploy models for specific tasks without remembering complex model names or URLs. This feature makes it easier to select the right model for your specific needs while abstracting away the complexity of model URLs and configurations.

<Tip>
  Usecases are predefined mappings between common AI tasks and the most suitable models for those tasks, optimized for your hardware configuration.
</Tip>

## Using Predefined Usecases

Instead of specifying a model directly, you can use a predefined usecase:

```bash
solo serve -u coding
```

This will automatically select the appropriate model for coding tasks based on your hardware. The system will:
- Detect your available hardware resources
- Select the most suitable model for the coding usecase
- Configure the appropriate backend (vLLM, Ollama, or Llama.cpp)
- Set up optimal parameters for the selected usecase

## Available Usecases

To see all available usecases:

```bash
solo list usecases
```

Common usecases include:

| Usecase | Description | Typical Models |
|---------|-------------|----------------|
| `coding` | Optimized for code generation and completion | CodeLlama, StarCoder |
| `chat` | General conversational AI | Llama, Mistral |
| `knowledge` | For RAG and knowledge-based applications | Mixtral, Llama-2 |
| `multilingual` | For non-English language support | Mixtral, Yi |

## Using Model Shortnames

Solo Server supports shortnames for models, making it easier to reference them:

```bash
solo serve -m llama3-8b
```

Instead of the full URL or path. This is particularly useful when:
- Working with frequently used models
- Sharing commands with team members
- Creating automation scripts
- Documenting workflows

<Tip>
  Shortnames are resolved to full transport URLs and can include metadata about the model's capabilities and requirements.
</Tip>

## Configuring Usecases and Shortnames

You can customize the usecase-to-model mapping by editing the `usecases.conf` file in your Solo Server installation directory. The configuration file supports:

```yaml
usecases:
  coding:
    default: "codellama-7b"
    fallback: "starcoder-3b"
  chat:
    default: "llama2-7b"
    fallback: "mistral-7b"

shortnames:
  llama3-8b: "https://huggingface.co/meta-llama/Llama-2-7b"
  codellama-7b: "https://huggingface.co/codellama/CodeLlama-7b"
```

### Configuration Options

| Option | Description | Example |
|--------|-------------|---------|
| `default` | Primary model for the usecase | `"codellama-7b"` |
| `fallback` | Backup model if primary is unavailable | `"starcoder-3b"` |
| `requirements` | Hardware requirements for the model | `{"gpu_memory": "8GB"}` |

## Best Practices

1. **Hardware Awareness**: The system automatically selects models based on your hardware capabilities
2. **Fallback Models**: Always configure fallback models for reliability
3. **Custom Shortnames**: Create meaningful shortnames for your frequently used models
4. **Version Control**: Keep your `usecases.conf` in version control for team consistency

<Note>
  When using usecases or shortnames, the system will automatically handle model downloading, quantization, and backend selection based on your hardware configuration.
</Note>

## Advanced Configuration

For more advanced usecases, you can specify additional parameters:

```yaml
usecases:
  coding:
    default: "codellama-7b"
    fallback: "starcoder-3b"
    parameters:
      temperature: 0.2
      max_tokens: 2048
      stop: ["```"]
  chat:
    default: "llama2-7b"
    fallback: "mistral-7b"
    parameters:
      temperature: 0.7
      max_tokens: 1024
```

<Warning>
  Advanced configurations may override the automatic hardware-aware settings. Use with caution.
</Warning> 