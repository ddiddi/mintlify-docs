---
title: 'Model Benchmarking'
description: 'How to benchmark models and interpret performance metrics'
---

## Overview

Solo Server includes a comprehensive benchmarking system to help you evaluate model performance on your hardware. This tool allows you to compare different models, backends, and configurations to find the optimal setup for your specific use case.

<Tip>
  Benchmarking is essential before deploying models in production to ensure optimal performance and resource utilization.
</Tip>

## Basic Benchmarking

Run a basic benchmark with default prompts:

```bash
solo benchmark -s ollama -m llama3
```

This command will:
- Load the specified model
- Run inference on default prompts
- Display performance metrics

<Frame>
  ```
  ╭──── Benchmark Results ────╮
  │ llama3                    │
  │ Response: 0.00 tokens/s   │
  │ Total: 0.00 tokens/s      │
  │                           │
  │ Stats:                    │
  │  - Response tokens: 0     │
  │  - Model load time: 0.00s │
  │  - Response time: 0.00s   │
  │  - Total time: 0.00s      │
  ╰───────────────────────────╯
  ```
</Frame>

## Custom Prompts

Test with your own prompts to evaluate performance on specific tasks:

```bash
solo benchmark -s vllm -m mistral -p "Explain quantum computing" -p "Write a Python function to sort a list"
```

You can specify multiple prompts to get a more comprehensive evaluation of model performance across different types of tasks.

## Understanding Metrics

The benchmark results include several key metrics:

| Metric | Description | What to Look For |
|--------|-------------|------------------|
| Response tokens/s | Tokens generated per second during inference | Higher is better |
| Total tokens/s | Overall throughput including model loading | Higher is better |
| Response tokens | Number of tokens in the response | Depends on prompt |
| Model load time | Time to load the model into memory | Lower is better |
| Response time | Time to generate the response | Lower is better |
| Total time | Total time including loading | Lower is better |

<Note>
  The "Response tokens/s" metric is particularly important for real-time applications where latency is critical.
</Note>

## Comparing Backends

To compare different backends, run benchmarks with the same model:

```bash
# Test with Ollama
solo benchmark -s ollama -m llama3

# Test with vLLM
solo benchmark -s vllm -m llama3

# Test with Llama.cpp
solo benchmark -s llama.cpp -m llama3
```

This allows you to determine which backend performs best for your specific hardware configuration.

## Hardware-Specific Considerations

Different hardware configurations will yield varying performance results:

- **GPU Memory**: Models that fit entirely in GPU memory will perform best with vLLM
- **CPU Cores**: More cores generally improve Llama.cpp performance
- **Memory Bandwidth**: Higher memory bandwidth improves all backends
- **Quantization**: Lower precision (4-bit vs 8-bit) trades accuracy for speed

<Warning>
  When benchmarking on shared hardware, be aware that other processes may affect your results. For the most accurate benchmarks, run on dedicated hardware when possible.
</Warning>

## Advanced Benchmarking Options

Solo Server's benchmarking system supports several advanced options:

```bash
# Specify number of iterations
solo benchmark -s vllm -m mistral --iterations 10

# Set maximum tokens for responses
solo benchmark -s ollama -m llama3 --max-tokens 512

# Save results to a file
solo benchmark -s llama.cpp -m llama3 --output results.json
```

## Interpreting Results

When comparing results, consider these factors:

1. **Speed vs. Quality Trade-off**: Faster inference often comes at the cost of model quality
2. **Use Case Requirements**: Different applications have different performance needs
3. **Hardware Constraints**: Your available resources will limit maximum performance
4. **Consistency**: Look for consistent performance across different prompts

<Frame>
  ```
  ╭──── Backend Comparison ────╮
  │ Backend  │ Tokens/s │ Load Time │
  │----------|----------|-----------│
  │ vLLM     │ 120.5    │ 2.3s      │
  │ Ollama   │ 85.2     │ 1.8s      │
  │ Llama.cpp│ 45.7     │ 0.5s      │
  ╰─────────────────────────────╯
  ```
</Frame>

## Troubleshooting

If you encounter issues during benchmarking:

- **Out of Memory Errors**: Try using a smaller model or enabling quantization
- **Slow Performance**: Check if other processes are consuming system resources
- **Inconsistent Results**: Increase the number of iterations for more reliable averages
- **Backend Errors**: Verify that the backend is properly installed and configured

## Best Practices

For the most accurate and useful benchmark results:

1. Run benchmarks on a clean system with minimal background processes
2. Test with multiple prompts that represent your actual use cases
3. Compare results across different model sizes and backends
4. Document your findings for future reference
5. Re-run benchmarks after system updates or configuration changes

<Note>
  Regular benchmarking helps you track performance improvements over time and identify potential regressions.
</Note> 